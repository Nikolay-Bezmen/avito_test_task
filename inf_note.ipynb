{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b11c0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∏–º–ø–æ—Ä—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a95b6994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.\n",
      "‚úÖ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è 'predict_all' –≥–æ—Ç–æ–≤–∞.\n",
      "‚úÖ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è 'post_process_punctuation' –≥–æ—Ç–æ–≤–∞.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# –®–ê–ì 3 (–û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ò –û–ß–ò–©–ï–ù–ù–´–ô)\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –í–°–ï–• –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ---\n",
    "MODEL_DIR = \"./rubert_tiny_spaced_restoration_best_f1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"‚úÖ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.\")\n",
    "except OSError:\n",
    "    print(f\"‚ùå –û–®–ò–ë–ö–ê: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –º–æ–¥–µ–ª—å—é '{MODEL_DIR}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!\")\n",
    "\n",
    "\n",
    "# --- –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ---\n",
    "def predict_all(text: str) -> tuple[str, list[int]]:\n",
    "    \"\"\"\n",
    "    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂ –∏–∑ –¥–≤—É—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤:\n",
    "    1. –¢–µ–∫—Å—Ç —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏.\n",
    "    2. –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –≤—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–æ–±–µ–ª.\n",
    "    \"\"\"\n",
    "    # –≠–¢–ê –ü–†–û–í–ï–†–ö–ê –ò–°–ü–†–ê–í–õ–Ø–ï–¢ –û–®–ò–ë–ö–£ INDEXERROR\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return \"\", []\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    inputs = tokenizer(text_lower, return_tensors=\"pt\", truncation=True, is_split_into_words=False)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    predictions = torch.argmax(logits, dim=2)[0].cpu().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0].cpu().numpy())\n",
    "\n",
    "    restored_text = \"\"\n",
    "    space_indices = []\n",
    "    current_char_index = 0\n",
    "    is_first_token = True\n",
    "\n",
    "    for token, prediction in zip(tokens, predictions):\n",
    "        if token in (tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token):\n",
    "            continue\n",
    "        \n",
    "        if prediction == 1 and not is_first_token:\n",
    "            restored_text += \" \"\n",
    "            space_indices.append(current_char_index)\n",
    "            \n",
    "        cleaned_token = token.replace('##', '')\n",
    "        restored_text += cleaned_token\n",
    "        current_char_index += len(cleaned_token)\n",
    "        is_first_token = False\n",
    "\n",
    "    return restored_text, space_indices\n",
    "\n",
    "print(\"‚úÖ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è 'predict_all' –≥–æ—Ç–æ–≤–∞.\")\n",
    "\n",
    "\n",
    "# --- –§—É–Ω–∫—Ü–∏—è –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ ---\n",
    "def post_process_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é —Ñ–∏–Ω–∞–ª—å–Ω—É—é —á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞:\n",
    "    1. –†–∞–∑–ª–∏—á–∞–µ—Ç —Ç–∏—Ä–µ –∏ –¥–µ—Ñ–∏—Å, –∏—Å–ø–æ–ª—å–∑—É—è —Å–ª–æ–≤–∞—Ä—å –∏—Å–∫–ª—é—á–µ–Ω–∏–π –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª–æ–≤.\n",
    "    2. –î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç.\n",
    "    \"\"\"\n",
    "    COMMON_HYPHENATED_WORDS = [\n",
    "        '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥', '–±–∏–∑–Ω–µ—Å-–ª–∞–Ω—á', '–ø—Ä–µ—Å—Å-–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', '–æ–Ω–ª–∞–π–Ω-—É—Ä–æ–∫',\n",
    "        '–≤–µ–±-—Å–∞–π—Ç', '—Å–µ—Ä–æ-–±—É—Ä–æ-–º–∞–ª–∏–Ω–æ–≤—ã–π', '–∫–æ–≥–¥–∞-–Ω–∏–±—É–¥—å', '–≥–¥–µ-–ª–∏–±–æ',\n",
    "        '—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É', '–∫—Ç–æ-—Ç–æ'\n",
    "    ]\n",
    "    HYPHEN_PARTICLES_PREFIX = ['–ø–æ', '–∫–æ–µ']\n",
    "    HYPHEN_PARTICLES_SUFFIX = ['—Ç–æ', '–ª–∏–±–æ', '–Ω–∏–±—É–¥—å', '–∫–∞', '—Ç–∞–∫–∏', '–¥–µ']\n",
    "\n",
    "    processed_text = re.sub(r'\\s*,\\s*', ', ', text)\n",
    "    processed_text = re.sub(r'\\s*-\\s*', ' - ', processed_text)\n",
    "\n",
    "    for word in COMMON_HYPHENATED_WORDS:\n",
    "        spaced_word = word.replace('-', ' - ')\n",
    "        processed_text = re.sub(spaced_word, word, processed_text, flags=re.IGNORECASE)\n",
    "\n",
    "    for suffix in HYPHEN_PARTICLES_SUFFIX:\n",
    "        pattern = re.compile(rf'(\\s-\\s)({suffix})\\b', re.IGNORECASE)\n",
    "        processed_text = pattern.sub(rf'-\\2', processed_text)\n",
    "    \n",
    "    for prefix in HYPHEN_PARTICLES_PREFIX:\n",
    "        pattern = re.compile(rf'\\b({prefix})(\\s-\\s)', re.IGNORECASE)\n",
    "        processed_text = pattern.sub(rf'\\1-', processed_text)\n",
    "        \n",
    "    return processed_text\n",
    "    \n",
    "print(\"‚úÖ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è 'post_process_punctuation' –≥–æ—Ç–æ–≤–∞.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "10a1b14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: './DeepPavlov/rubert-base-cased_spaced_restoration_20_epoch_16_batch_0.1_0.00002'\n",
      "  -> –ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏. –£–¥–∞–ª—è–µ–º...\n",
      "  -> –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é './DeepPavlov/rubert-base-cased_spaced_restoration_20_epoch_16_batch_0.1_0.00002'...\n",
      "\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ './DeepPavlov/rubert-base-cased_spaced_restoration_20_epoch_16_batch_0.1_0.00002' –∏ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ –¥—Ä—É–≥–æ–º –Ω–æ—É—Ç–±—É–∫–µ.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# –®–∞–≥ 7: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫\n",
    "# ==============================================================================\n",
    "\n",
    "# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–¥–æ–ª–∂–Ω–∞ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ç–æ–π, —á—Ç–æ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)\n",
    "OUTPUT_MODEL_DIR = \"./DeepPavlov/rubert-base-cased_spaced_restoration_20_epoch_16_batch_0.1_0.00002\"\n",
    "\n",
    "print(f\"üíæ–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: '{OUTPUT_MODEL_DIR}'\")\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º `os` –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç.\n",
    "# –ê —Ç–∞–∫–∂–µ `shutil` –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤.\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(OUTPUT_MODEL_DIR):\n",
    "    print(\"  -> –ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏. –£–¥–∞–ª—è–µ–º...\")\n",
    "    shutil.rmtree(OUTPUT_MODEL_DIR)\n",
    "\n",
    "print(f\"  -> –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é '{OUTPUT_MODEL_DIR}'...\")\n",
    "os.makedirs(OUTPUT_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å. –ú–µ—Ç–æ–¥ `save_pretrained` —Å–æ—Ö—Ä–∞–Ω–∏—Ç –∏ –≤–µ—Å–∞ (pytorch_model.bin),\n",
    "# –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª (config.json).\n",
    "model.save_pretrained(OUTPUT_MODEL_DIR)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –≠—Ç–æ –Ω–µ –º–µ–Ω–µ–µ –≤–∞–∂–Ω–æ! –û–Ω —Å–æ—Ö—Ä–∞–Ω–∏—Ç —Å–≤–æ–π —Å–ª–æ–≤–∞—Ä—å\n",
    "# (vocab.txt) –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã.\n",
    "tokenizer.save_pretrained(OUTPUT_MODEL_DIR)\n",
    "\n",
    "print(f\"\\n‚úÖ –ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ '{OUTPUT_MODEL_DIR}' –∏ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ –¥—Ä—É–≥–æ–º –Ω–æ—É—Ç–±—É–∫–µ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ab470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (–≤–∫–ª—é—á–∞—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫—É) —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_no_spaces</th>\n",
       "      <th>restored_text</th>\n",
       "      <th>predicted_positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–∫—É–ø–ª—é–∞–π—Ñ–æ–Ω14–ø—Ä–æ</td>\n",
       "      <td>–∫—É–ø–ª—é –∞–π—Ñ–æ–Ω 14–ø—Ä–æ</td>\n",
       "      <td>[5, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>–∏—â—É–¥–æ–º–≤–ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ</td>\n",
       "      <td>–∏—â—É–¥–æ–º–≤–ø–æ–¥ –º–æ—Å–∫–æ–≤ —å–µ</td>\n",
       "      <td>[10, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>—Å–¥–∞—é–∫–≤–∞—Ä—Ç–∏—Ä—É—Å–º–µ–±–µ–ª—å—é–∏—Ç–µ—Ö–Ω–∏–∫–æ–π</td>\n",
       "      <td>—Å–¥–∞ —é–∫–≤–∞—Ä—Ç–∏—Ä —É—Å–º–µ–±–µ–ª—å —é–∏—Ç–µ—Ö–Ω–∏–∫–æ–π</td>\n",
       "      <td>[3, 11, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>–Ω–æ–≤—ã–π–¥–∏–≤–∞–Ω–¥–æ—Å—Ç–∞–≤–∫–∞–Ω–µ–¥–æ—Ä–æ–≥–æ</td>\n",
       "      <td>–Ω–æ–≤—ã–π–¥–∏–≤–∞–Ω–¥–æ—Å—Ç–∞–≤–∫–∞–Ω –µ–¥ –æ—Ä–æ–≥–æ</td>\n",
       "      <td>[19, 21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–æ—Ç–¥–∞–º–¥–∞—Ä–æ–º–∫–æ—à–∫—É</td>\n",
       "      <td>–æ—Ç–¥–∞–º–¥–∞—Ä–æ–º –∫–æ—à–∫—É</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>–Ø–Ω–µ—É—Å–Ω—É.</td>\n",
       "      <td>—è–Ω–µ—É —Å–Ω—É .</td>\n",
       "      <td>[4, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1001</td>\n",
       "      <td>–í–µ—Å–Ω–∞-—è—É–∂–µ–Ω–µ–≥—Ä–µ—é–ø–∏–æ.</td>\n",
       "      <td>–≤–µ—Å–Ω–∞ - —è—É–∂–µ–Ω–µ–≥—Ä–µ —é–ø–∏ –æ .</td>\n",
       "      <td>[15, 18, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1002</td>\n",
       "      <td>–í–µ—Å–Ω–∞-—Å–∫–æ—Ä–æ–≤—ã—Ä–∞—Å—Ç–µ—Ç—Ç—Ä–∞–≤–∞.</td>\n",
       "      <td>–≤–µ—Å–Ω–∞ - —Å–∫–æ—Ä–æ–≤—ã —Ä–∞—Å—Ç–µ—Ç—Ç —Ä–∞–≤–∞ .</td>\n",
       "      <td>[13, 20, 24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1003</td>\n",
       "      <td>–í–µ—Å–Ω–∞-–≤—ã–ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ,–∫–∞–∫–∫—Ä–∞—Å–∏–≤–æ.</td>\n",
       "      <td>–≤–µ—Å–Ω–∞ - –≤—ã–ø–æ—Å–º–æ—Ç—Ä –∏—Ç–µ, –∫–∞–∫ –∫—Ä–∞—Å –∏–≤–æ .</td>\n",
       "      <td>[6, 15, 19, 22, 26, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1004</td>\n",
       "      <td>–í–µ—Å–Ω–∞-–≥–¥–µ–º–æ—è–≥–æ–ª–æ–≤–∞?</td>\n",
       "      <td>–≤–µ—Å–Ω–∞ - –≥–¥–µ–º–æ—è–≥–æ –ª–æ–≤–∞ ?</td>\n",
       "      <td>[6, 14, 18]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                  text_no_spaces  \\\n",
       "0        0                 –∫—É–ø–ª—é–∞–π—Ñ–æ–Ω14–ø—Ä–æ   \n",
       "1        1              –∏—â—É–¥–æ–º–≤–ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ   \n",
       "2        2   —Å–¥–∞—é–∫–≤–∞—Ä—Ç–∏—Ä—É—Å–º–µ–±–µ–ª—å—é–∏—Ç–µ—Ö–Ω–∏–∫–æ–π   \n",
       "3        3      –Ω–æ–≤—ã–π–¥–∏–≤–∞–Ω–¥–æ—Å—Ç–∞–≤–∫–∞–Ω–µ–¥–æ—Ä–æ–≥–æ   \n",
       "4        4                 –æ—Ç–¥–∞–º–¥–∞—Ä–æ–º–∫–æ—à–∫—É   \n",
       "...    ...                             ...   \n",
       "1000  1000                        –Ø–Ω–µ—É—Å–Ω—É.   \n",
       "1001  1001            –í–µ—Å–Ω–∞-—è—É–∂–µ–Ω–µ–≥—Ä–µ—é–ø–∏–æ.   \n",
       "1002  1002       –í–µ—Å–Ω–∞-—Å–∫–æ—Ä–æ–≤—ã—Ä–∞—Å—Ç–µ—Ç—Ç—Ä–∞–≤–∞.   \n",
       "1003  1003  –í–µ—Å–Ω–∞-–≤—ã–ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ,–∫–∞–∫–∫—Ä–∞—Å–∏–≤–æ.   \n",
       "1004  1004             –í–µ—Å–Ω–∞-–≥–¥–µ–º–æ—è–≥–æ–ª–æ–≤–∞?   \n",
       "\n",
       "                              restored_text      predicted_positions  \n",
       "0                         –∫—É–ø–ª—é –∞–π—Ñ–æ–Ω 14–ø—Ä–æ                  [5, 10]  \n",
       "1                      –∏—â—É–¥–æ–º–≤–ø–æ–¥ –º–æ—Å–∫–æ–≤ —å–µ                 [10, 16]  \n",
       "2          —Å–¥–∞ —é–∫–≤–∞—Ä—Ç–∏—Ä —É—Å–º–µ–±–µ–ª—å —é–∏—Ç–µ—Ö–Ω–∏–∫–æ–π              [3, 11, 19]  \n",
       "3              –Ω–æ–≤—ã–π–¥–∏–≤–∞–Ω–¥–æ—Å—Ç–∞–≤–∫–∞–Ω –µ–¥ –æ—Ä–æ–≥–æ                 [19, 21]  \n",
       "4                          –æ—Ç–¥–∞–º–¥–∞—Ä–æ–º –∫–æ—à–∫—É                     [10]  \n",
       "...                                     ...                      ...  \n",
       "1000                             —è–Ω–µ—É —Å–Ω—É .                   [4, 7]  \n",
       "1001              –≤–µ—Å–Ω–∞ - —è—É–∂–µ–Ω–µ–≥—Ä–µ —é–ø–∏ –æ .             [15, 18, 19]  \n",
       "1002         –≤–µ—Å–Ω–∞ - —Å–∫–æ—Ä–æ–≤—ã —Ä–∞—Å—Ç–µ—Ç—Ç —Ä–∞–≤–∞ .             [13, 20, 24]  \n",
       "1003  –≤–µ—Å–Ω–∞ - –≤—ã–ø–æ—Å–º–æ—Ç—Ä –∏—Ç–µ, –∫–∞–∫ –∫—Ä–∞—Å –∏–≤–æ .  [6, 15, 19, 22, 26, 29]  \n",
       "1004                –≤–µ—Å–Ω–∞ - –≥–¥–µ–º–æ—è–≥–æ –ª–æ–≤–∞ ?              [6, 14, 18]  \n",
       "\n",
       "[1005 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# –®–∞–≥ 4: –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ DataFrame\n",
    "# ==============================================================================\n",
    "\n",
    "TEST_FILE_NAME = \"text_data.txt\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    ids_list = []\n",
    "    original_texts_list = []\n",
    "    restored_texts_list = []\n",
    "    predicted_indices_list = []\n",
    "\n",
    "    # –ß–∏—Ç–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –ø–æ—Å—Ç—Ä–æ—á–Ω–æ\n",
    "    with open(TEST_FILE_NAME, 'r', encoding='utf-8') as f:\n",
    "        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å\n",
    "        next(f, None)\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # –†–∞–∑–¥–µ–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –ø–æ –ø–µ—Ä–≤–æ–π –∑–∞–ø—è—Ç–æ–π –Ω–∞ 'id' –∏ '—Ç–µ–∫—Å—Ç'\n",
    "                line_id, text_no_spaces = line.split(',', 1)\n",
    "\n",
    "                # 1. –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏\n",
    "                restored_text_from_model, space_indices = predict_all(text_no_spaces)\n",
    "                # 2. –ü–†–ò–ú–ï–ù–Ø–ï–ú –ü–û–°–¢-–û–ë–†–ê–ë–û–¢–ö–£\n",
    "                final_restored_text = post_process_punctuation(restored_text_from_model)\n",
    "\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–ø–∏—Å–∫–∏\n",
    "                ids_list.append(line_id)\n",
    "                original_texts_list.append(text_no_spaces)\n",
    "                restored_texts_list.append(final_restored_text)\n",
    "                predicted_indices_list.append(space_indices)\n",
    "\n",
    "            except ValueError:\n",
    "                print(f\"‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –°—Ç—Ä–æ–∫–∞ '{line}' –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç—É 'id,text' –∏ –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–∞.\")\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π DataFrame –∏–∑ —Å–ø–∏—Å–∫–æ–≤\n",
    "    if ids_list:\n",
    "        results_df = pd.DataFrame({\n",
    "            'id': ids_list,\n",
    "            'text_no_spaces': original_texts_list,\n",
    "            'restored_text': restored_texts_list,\n",
    "            'predicted_positions': predicted_indices_list\n",
    "        })\n",
    "        \n",
    "        print(\"\\n‚úÖ DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (–≤–∫–ª—é—á–∞—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫—É) —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω:\")\n",
    "        \n",
    "        pd.set_option('display.max_colwidth', 120)\n",
    "        display(results_df)\n",
    "\n",
    "    else:\n",
    "        print(\"‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –í —Ñ–∞–π–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.\")\n",
    "\n",
    "\n",
    "except NameError as e:\n",
    "    if 'post_process_punctuation' in str(e):\n",
    "         print(\"‚ùå –û–®–ò–ë–ö–ê: –§—É–Ω–∫—Ü–∏—è 'post_process_punctuation' –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –≤—ã–ø–æ–ª–Ω–∏–ª–∏ —è—á–µ–π–∫—É —Å –µ—ë –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º.\")\n",
    "    elif 'predict_all' in str(e):\n",
    "        print(\"‚ùå –û–®–ò–ë–ö–ê: –§—É–Ω–∫—Ü–∏—è 'predict_all' –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –≤—ã–ø–æ–ª–Ω–∏–ª–∏ —è—á–µ–π–∫—É —Å –µ—ë –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º.\")\n",
    "    else:\n",
    "        print(f\"‚ùå –û–®–ò–ë–ö–ê: {e}. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —è—á–µ–π–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª '{TEST_FILE_NAME}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa46571",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df[['id', 'predicted_positions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a7afbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24daaf38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted_positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>150</td>\n",
       "      <td>[25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>151</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>152</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>153</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>154</td>\n",
       "      <td>[6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>155</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>156</td>\n",
       "      <td>[9, 17, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>157</td>\n",
       "      <td>[3, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>158</td>\n",
       "      <td>[10, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>[22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>161</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>[11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>[7, 8, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>164</td>\n",
       "      <td>[18, 21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>165</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>166</td>\n",
       "      <td>[3, 7, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>167</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>168</td>\n",
       "      <td>[6, 10, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>169</td>\n",
       "      <td>[10, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>170</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>171</td>\n",
       "      <td>[12, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>[13, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>173</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>174</td>\n",
       "      <td>[8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>175</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>176</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>177</td>\n",
       "      <td>[14, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>178</td>\n",
       "      <td>[8, 10, 11, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>179</td>\n",
       "      <td>[6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>180</td>\n",
       "      <td>[22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>181</td>\n",
       "      <td>[4, 13, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>182</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>183</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>184</td>\n",
       "      <td>[5, 9, 11, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>185</td>\n",
       "      <td>[11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>186</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>187</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>188</td>\n",
       "      <td>[12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>189</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>190</td>\n",
       "      <td>[5, 10, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>191</td>\n",
       "      <td>[8, 10, 14, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>192</td>\n",
       "      <td>[10, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>193</td>\n",
       "      <td>[15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>194</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>[19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>[13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>[9, 13, 15, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id predicted_positions\n",
       "150  150                [25]\n",
       "151  151                  []\n",
       "152  152                  []\n",
       "153  153                  []\n",
       "154  154                 [6]\n",
       "155  155                  []\n",
       "156  156         [9, 17, 19]\n",
       "157  157             [3, 14]\n",
       "158  158            [10, 12]\n",
       "159  159                [22]\n",
       "160  160                  []\n",
       "161  161                  []\n",
       "162  162                [11]\n",
       "163  163          [7, 8, 13]\n",
       "164  164            [18, 21]\n",
       "165  165                [14]\n",
       "166  166          [3, 7, 15]\n",
       "167  167                  []\n",
       "168  168         [6, 10, 14]\n",
       "169  169            [10, 12]\n",
       "170  170                  []\n",
       "171  171            [12, 15]\n",
       "172  172            [13, 15]\n",
       "173  173                  []\n",
       "174  174                 [8]\n",
       "175  175                  []\n",
       "176  176                [10]\n",
       "177  177            [14, 19]\n",
       "178  178     [8, 10, 11, 13]\n",
       "179  179                 [6]\n",
       "180  180                [22]\n",
       "181  181         [4, 13, 19]\n",
       "182  182                  []\n",
       "183  183                  []\n",
       "184  184      [5, 9, 11, 12]\n",
       "185  185                [11]\n",
       "186  186                  []\n",
       "187  187                  []\n",
       "188  188                [12]\n",
       "189  189                  []\n",
       "190  190         [5, 10, 12]\n",
       "191  191     [8, 10, 14, 20]\n",
       "192  192            [10, 16]\n",
       "193  193                [15]\n",
       "194  194                  []\n",
       "195  195                [19]\n",
       "196  196                [13]\n",
       "197  197                [17]\n",
       "198  198     [9, 13, 15, 18]\n",
       "199  199                  []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[150:200]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
