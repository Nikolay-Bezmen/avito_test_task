{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    CanineTokenizer,\n",
    "    CanineForTokenClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Tuple\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Константы и Настройка\n",
    "# =======================\n",
    "MODEL_NAME = \"google/canine-c\"\n",
    "DATASET_NAME = \"dgramus/synth-ecom-search-queries\"\n",
    "AUGMENTED_DATA_FILE = \"augmented_test_data.csv\"\n",
    "AUGMENTED_DATA_FILE_TXT = \"augmented_add_train.txt\"\n",
    "BEST_MODEL_DIR = \"./final_corrected_model\"\n",
    "\n",
    "LABEL_LIST = [\"NO_SPACE\", \"SPACE\"]\n",
    "LABEL_TO_ID = {label: i for i, label in enumerate(LABEL_LIST)}\n",
    "ID_TO_LABEL = {v: k for k, v in LABEL_TO_ID.items()}\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "CHAR_WINDOW = MAX_LENGTH - 2\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 6\n",
    "LEARNING_RATE = 3e-5\n",
    "SEED = 42\n",
    "\n",
    "if os.path.exists(BEST_MODEL_DIR):\n",
    "    shutil.rmtree(BEST_MODEL_DIR)\n",
    "os.makedirs(BEST_MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca64f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется устройство: mps\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Фиксируем сиды и устройство\n",
    "# =======================\n",
    "def set_seed(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    import numpy as np\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available(): device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
    "else: device = torch.device(\"cpu\")\n",
    "print(f\"Используется устройство: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# ЕДИНАЯ И ПРАВИЛЬНАЯ ЛОГИКА РАСЧЕТА ПОЗИЦИЙ\n",
    "# =======================\n",
    "def get_true_positions(no_space_text: str, corrected_text: str) -> List[int]:\n",
    "    if not isinstance(no_space_text, str) or not isinstance(corrected_text, str):\n",
    "        return []\n",
    "\n",
    "    ns_text = \"\".join(no_space_text.split()).lower()\n",
    "    corr_text = \" \".join(corrected_text.split()).lower()\n",
    "\n",
    "    positions = []\n",
    "    ns_idx = 0\n",
    "    corr_idx = 0\n",
    "\n",
    "    while ns_idx < len(ns_text) and corr_idx < len(corr_text):\n",
    "        if ns_text[ns_idx] == corr_text[corr_idx]:\n",
    "            ns_idx += 1\n",
    "            corr_idx += 1\n",
    "        elif corr_text[corr_idx] == ' ':\n",
    "            # Мы нашли пробел. Это значит, что перед символом ns_text[ns_idx] должен быть пробел.\n",
    "            if ns_idx > 0: # Пробел не может быть в самом начале\n",
    "                positions.append(ns_idx)\n",
    "            corr_idx += 1 # Пропускаем пробел в corrected_text\n",
    "        else:\n",
    "            # Символы не совпадают, но это не пробел. Синхронизируемся.\n",
    "            ns_idx += 1\n",
    "            corr_idx += 1\n",
    "    return positions\n",
    "\n",
    "def compute_f1_metric(all_true_positions: List[List[int]], all_pred_positions: List[List[int]]) -> float:\n",
    "    f1_scores = []\n",
    "    for true_pos, pred_pos in zip(all_true_positions, all_pred_positions):\n",
    "        true_set, pred_set = set(true_pos), set(pred_pos)\n",
    "        tp = len(true_set & pred_set)\n",
    "        precision = tp / len(pred_set) if len(pred_set) > 0 else 0.0\n",
    "        recall = tp / len(true_set) if len(true_set) > 0 else (1.0 if len(pred_set) == 0 else 0.0)\n",
    "        denom = precision + recall\n",
    "        f1 = (2 * precision * recall) / denom if denom > 0 else 0.0\n",
    "        f1_scores.append(f1)\n",
    "    return sum(f1_scores) / len(f1_scores) if f1_scores else 0.0\n",
    "\n",
    "def restore_spaces(text: str, model, tokenizer, device) -> List[int]:\n",
    "    \"\"\"Возвращает только список предсказанных позиций.\"\"\"\n",
    "    model.eval()\n",
    "    clean_text = \"\".join((text or \"\")).replace(\" \", \"\").lower()\n",
    "    if not clean_text: return []\n",
    "    space_positions, offset = [], 0\n",
    "    while offset < len(clean_text):\n",
    "        chunk = clean_text[offset : offset + CHAR_WINDOW]\n",
    "        encoding = tokenizer(chunk, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.argmax(model(**encoding).logits, dim=-1).squeeze(0)\n",
    "        preds_chars = preds[1 : 1 + len(chunk)].cpu().tolist()\n",
    "        for i, char in enumerate(chunk):\n",
    "            if (offset + i > 0) and preds_chars[i] == LABEL_TO_ID[\"SPACE\"]:\n",
    "                space_positions.append(offset + i)\n",
    "        offset += len(chunk)\n",
    "    return space_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cabf647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Препроцессинг и Обучение\n",
    "# =======================\n",
    "tokenizer = CanineTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    input_texts = examples.get(\"query\", examples.get(\"text_no_spaces\"))\n",
    "    texts_no_spaces = [\"\".join(t.split()).lower() if isinstance(t, str) else \"\" for t in input_texts]\n",
    "    encodings = tokenizer(texts_no_spaces, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "    \n",
    "    true_positions_list = [get_true_positions(ns, corr) for ns, corr in zip(texts_no_spaces, examples[\"query_corrected\"])]\n",
    "    processed_labels = []\n",
    "    for positions, ns in zip(true_positions_list, texts_no_spaces):\n",
    "        char_labels = [LABEL_TO_ID['NO_SPACE']] * len(ns[:CHAR_WINDOW])\n",
    "        for pos in positions:\n",
    "            if pos < len(char_labels):\n",
    "                char_labels[pos] = LABEL_TO_ID['SPACE']\n",
    "        labels = [-100] + char_labels + [-100]\n",
    "        labels.extend([-100] * (MAX_LENGTH - len(labels)))\n",
    "        processed_labels.append(labels[:MAX_LENGTH])\n",
    "    encodings[\"labels\"] = processed_labels\n",
    "    return encodings\n",
    "\n",
    "def evaluate(model, dataloader, loss_fct, device):\n",
    "    \"\"\"Оценка во время обучения, использует ту же правильную логику.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, all_true_positions, all_pred_positions = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch[\"labels\"]\n",
    "            outputs = model(**{k: v for k, v in batch.items() if k != 'labels'})\n",
    "            loss = loss_fct(outputs.logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            for i in range(labels.shape[0]):\n",
    "                active_mask = labels[i] != -100\n",
    "                active_true = labels[i][active_mask].cpu().numpy()\n",
    "                active_pred = predictions[i][active_mask].cpu().numpy()\n",
    "                all_true_positions.append([idx for idx, lab in enumerate(active_true) if lab == LABEL_TO_ID[\"SPACE\"]])\n",
    "                all_pred_positions.append([idx for idx, lab in enumerate(active_pred) if lab == LABEL_TO_ID[\"SPACE\"]])\n",
    "    return total_loss / max(1, len(dataloader)), compute_f1_metric(all_true_positions, all_pred_positions)\n",
    "\n",
    "def train_model(train_dataset, eval_dataset, best_model_dir):\n",
    "    train_dataset.set_format(\"torch\")\n",
    "    eval_dataset.set_format(\"torch\")\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    eval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    model = CanineForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_LIST), id2label=ID_TO_LABEL, label2id=LABEL_TO_ID).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=max(1, num_training_steps // 10), num_training_steps=num_training_steps)\n",
    "    \n",
    "    all_labels_flat = [lab for ex in train_dataset for lab in ex['labels'] if lab != -100]\n",
    "    class_counts = np.bincount(all_labels_flat, minlength=len(LABEL_LIST))\n",
    "    class_weights = class_counts.sum() / (len(LABEL_LIST) * np.clip(class_counts, 1, None))\n",
    "    loss_fct = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float, device=device), ignore_index=-100)\n",
    "    \n",
    "    best_eval_f1 = -1.0\n",
    "    best_eval_loss = 2970471274\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [train]\")\n",
    "        for batch in pbar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**{k: v for k, v in batch.items() if k != 'labels'})\n",
    "            loss = loss_fct(outputs.logits.view(-1, model.config.num_labels), batch[\"labels\"].view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step(); lr_scheduler.step(); optimizer.zero_grad()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "        eval_loss, eval_f1 = evaluate(model, eval_dataloader, loss_fct, device)\n",
    "        print(f\"\\n--- Эпоха {epoch}/{NUM_EPOCHS} ---\")\n",
    "        print(f\"Eval loss: {eval_loss:.4f}, Eval F1: {eval_f1:.4f}\")\n",
    "        if eval_f1 > best_eval_f1:\n",
    "            print(f\"Новый лучший F1. Сохраняем модель в {best_model_dir}...\")\n",
    "            best_eval_f1 = eval_f1\n",
    "            model.save_pretrained(best_model_dir)\n",
    "            tokenizer.save_pretrained(best_model_dir)\n",
    "\n",
    "        if eval_loss < best_eval_loss:\n",
    "            print(f\"Новый лучший Loss eval. Сохраняем модель в {best_model_dir}_loss...\")\n",
    "            best_eval_loss = eval_loss\n",
    "            model.save_pretrained(best_model_dir + \"_loss\")\n",
    "            tokenizer.save_pretrained(best_model_dir + \"_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877e320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка и подготовка данных для обучения...\n",
      "Найден файл с дополнительными данными: augmented_test_data.csv\n",
      "Итоговый размер обучающего набора: 30069\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# ОСНОВНОЙ ПАЙПЛАЙН\n",
    "# =======================\n",
    "# --- 1. Подготовка данных ---\n",
    "print(\"Загрузка и подготовка данных для обучения...\")\n",
    "raw_dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "split_dataset = raw_dataset.train_test_split(test_size=0.1, seed=SEED)\n",
    "base_train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset_hf = split_dataset[\"test\"]\n",
    "\n",
    "if os.path.exists(AUGMENTED_DATA_FILE_TXT):\n",
    "    with open(AUGMENTED_DATA_FILE_TXT, 'r', encoding='utf-8') as f:\n",
    "        query_corrected = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "    query = [\"\".join(s.split()) for s in query_corrected]\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"query\" : query,\n",
    "        \"query_corrected\" : query_corrected\n",
    "    }\n",
    "    print(f\"Найден файл с дополнительными данными: {AUGMENTED_DATA_FILE}\")\n",
    "    lyrics_dataset = Dataset.from_dict(dataset_dict)\n",
    "    train_raw_dataset = concatenate_datasets([base_train_dataset, lyrics_dataset])\n",
    "else:\n",
    "    print(f\"Файл {AUGMENTED_DATA_FILE_TXT} не найден. Используйте только базовый датасет.\")\n",
    "    train_raw_dataset = base_train_dataset\n",
    "\n",
    "print(f\"Итоговый размер обучающего набора: {len(train_raw_dataset)}\")\n",
    "train_dataset = train_raw_dataset.map(preprocess_function, batched=True, remove_columns=train_raw_dataset.column_names)\n",
    "eval_dataset = eval_dataset_hf.map(preprocess_function, batched=True, remove_columns=eval_dataset_hf.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Переобучение модели с правильной логикой ---\n",
    "train_model(train_dataset, eval_dataset, BEST_MODEL_DIR)\n",
    "\n",
    "# --- 3. Оценка на тестовых данных ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- ОЦЕНКА ОБУЧЕННОЙ МОДЕЛИ НА РАЗМЕЧЕННОМ ТЕСТЕ ---\")\n",
    "\n",
    "print(f\"Загрузка лучшей модели из '{BEST_MODEL_DIR}'...\")\n",
    "model = CanineForTokenClassification.from_pretrained(BEST_MODEL_DIR).to(device)\n",
    "tokenizer = CanineTokenizer.from_pretrained(BEST_MODEL_DIR)\n",
    "\n",
    "if os.path.exists(AUGMENTED_DATA_FILE):\n",
    "    df_test = pd.read_csv(AUGMENTED_DATA_FILE)\n",
    "    all_true_positions, all_pred_positions = [], []\n",
    "\n",
    "    for _, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Финальная оценка\"):\n",
    "        pred_positions = restore_spaces(str(row['text_no_spaces']), model, tokenizer, device)\n",
    "        true_positions = get_true_positions(str(row['text_no_spaces']), str(row['query_corrected']))\n",
    "        all_pred_positions.append(pred_positions)\n",
    "        all_true_positions.append(true_positions)\n",
    "\n",
    "    final_f1_score = compute_f1_metric(all_true_positions, all_pred_positions)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\" ✅ Итоговый средний F1-score: {final_f1_score:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    print(f\"Файл {AUGMENTED_DATA_FILE} не найден для финальной оценки.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dbde68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- ОЦЕНКА ОБУЧЕННОЙ МОДЕЛИ НА ЛОКАЛЬНОМ ТЕСТЕ ---\")\n",
    "model = CanineForTokenClassification.from_pretrained(BEST_MODEL_DIR).to(device)\n",
    "tokenizer = CanineTokenizer.from_pretrained(BEST_MODEL_DIR)\n",
    "\n",
    "if os.path.exists(AUGMENTED_DATA_FILE):\n",
    "    df_test = pd.read_csv(AUGMENTED_DATA_FILE)\n",
    "    all_true_pos, all_pred_pos = [], []\n",
    "    for _, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Локальная оценка\"):\n",
    "        all_pred_pos.append(restore_spaces(str(row['text_no_spaces']), model, tokenizer, device))\n",
    "        all_true_pos.append(get_true_positions(str(row['text_no_spaces']), str(row['query_corrected'])))\n",
    "    final_f1_score = compute_f1_metric(all_true_pos, all_pred_pos)\n",
    "    print(f\" ✅ Итоговый средний F1-score на локальном тесте: {final_f1_score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- ГЕНЕРАЦИЯ ФАЙЛА ДЛЯ КОНКУРСА ---\")\n",
    "\n",
    "CONTEST_FILE = \"test_data.txt\" \n",
    "SUBMISSION_FILE = \"submission.csv\"\n",
    "\n",
    "def read_contest_file(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path): raise FileNotFoundError(f\"Файл не найден: {path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=None, engine=\"python\", encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        data_rows = []\n",
    "        with open(path, 'r', encoding='utf-8-sig') as f:\n",
    "            header = [h.strip() for h in f.readline().strip().split(',')]\n",
    "            for line in f:\n",
    "                if line.strip(): data_rows.append(line.strip().split(',', 1))\n",
    "        df = pd.DataFrame(data_rows, columns=header[:len(data_rows[0])])\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    if \"text_no_spaces\" not in df.columns: raise ValueError(\"Нужна колонка 'text_no_spaces'.\")\n",
    "    return df\n",
    "\n",
    "task_data = read_contest_file(CONTEST_FILE)\n",
    "\n",
    "all_positions_str = []\n",
    "for text in tqdm(task_data[\"text_no_spaces\"].astype(str), desc=\"Генерация сабмита\"):\n",
    "    positions = restore_spaces(text, model, tokenizer, device)\n",
    "    # Преобразуем список [5, 13] в строку \"[5, 13]\"\n",
    "    all_positions_str.append(str(positions))\n",
    "\n",
    "submission_df = task_data.copy()\n",
    "submission_df[\"predicted_positions\"] = all_positions_str\n",
    "submission_df = submission_df[[\"id\",\"predicted_positions\"]]\n",
    "submission_df.to_csv(SUBMISSION_FILE)\n",
    "print(f\"✅ Файл для отправки сохранен: {SUBMISSION_FILE}\")\n",
    "print(\"Пример первых 5 строк:\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
