{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69a50fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Зависимости успешно установлены и импортированы.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Шаг 1: Установка и импорт зависимостей\n",
    "# ==============================================================================\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_from_disk, load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from tqdm.notebook import tqdm\n",
    "import evaluate\n",
    "\n",
    "print(\"✅ Зависимости успешно установлены и импортированы.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee260b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-labextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q ipywidgets\n",
    "\n",
    "# 2. Включаем расширение для Jupyter Notebook (если вы работаете в нем)\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# 3. Включаем расширение для Jupyter Lab (если вы работаете в нем)\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0715cbf",
   "metadata": {},
   "source": [
    "## Шаг 2: Настройка конфигурации и утилит\n",
    "\n",
    "В этой ячейке определяются ключевые параметры и вспомогательные функции, которые будут использоваться на последующих этапах работы с данными и моделью. Здесь задаются пути, названия моделей и словари для меток.\n",
    "\n",
    "### Основные параметры\n",
    "\n",
    "*   `SOURCE_DATASET_NAME`: `\"dgramus/synth-ecom-search-queries\"`\n",
    "    *   **Описание:** Название исходного набора данных, который будет загружен из репозитория Hugging Face. В данном случае это датасет с синтетическими поисковыми запросами для e-commerce.\n",
    "\n",
    "*   `MODEL_CHECKPOINT`: `\"cointegrated/rubert-tiny2\"`\n",
    "    *   **Описание:** Идентификатор предварительно обученной модели-трансформера, которая будет использоваться в качестве основы. `rubert-tiny2` — это легковесная и быстрая версия модели BERT для русского языка.\n",
    "\n",
    "*   `OUTPUT_DATASET_DIR`: `\"./processed_dataset_for_bert\"`\n",
    "    *   **Описание:** Путь к локальной директории, куда будет сохранен обработанный и подготовленный для обучения датасет.\n",
    "\n",
    "### Метки для классификации\n",
    "\n",
    "*   `id2label` и `label2id`\n",
    "    *   **Описание:** Два словаря, которые обеспечивают взаимно-однозначное соответствие между числовыми идентификаторами (`0`, `1`) и строковыми метками (`\"CONTINUE\"`, `\"BEGIN\"`). Это стандартный подход для задач классификации, позволяющий легко преобразовывать предсказания модели в читаемый формат и обратно. Такие метки типичны для задач NER (Named Entity Recognition), где `BEGIN` отмечает начало сущности, а `CONTINUE` — её продолжение.\n",
    "\n",
    "### Вспомогательная функция\n",
    "\n",
    "#### Функция `clean_text`\n",
    "Эта функция предназначена для предварительной обработки текстовых данных.\n",
    "\n",
    "*   **Назначение:** Очистка текста от лишних пробельных символов.\n",
    "*   **Логика работы:**\n",
    "    1.  Проверяет, является ли входное значение строкой. Если нет, возвращает пустую строку.\n",
    "    2.  С помощью регулярного выражения `\\s+` заменяет один или несколько пробельных символов (пробелы, табы, переносы строк) на один пробел.\n",
    "    3.  Методом `.strip()` удаляет пробелы в начале и конце строки.\n",
    "\n",
    "> **Примечание:** Докстринг функции (комментарий внутри неё) утверждает, что функция также приводит текст к нижнему регистру и удаляет спецсимволы, однако в представленной реализации выполняется **только нормализация пробелов**. Это может быть важным моментом для дальнейшей отладки.\n",
    "\n",
    "### Завершение\n",
    "\n",
    "В конце ячейки выводится сообщение `\"Конфигурация и функция очистки готовы.\"`, которое подтверждает, что код был выполнен без ошибок и все переменные и функции успешно определены в текущей сессии.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8102ea5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Конфигурация и функция очистки готовы.\n"
     ]
    }
   ],
   "source": [
    "# Исходный датасет с e-commerce запросами\n",
    "SOURCE_DATASET_NAME = \"dgramus/synth-ecom-search-queries\"\n",
    "MODEL_CHECKPOINT = \"cointegrated/rubert-tiny2\"\n",
    "OUTPUT_DATASET_DIR = \"./processed_dataset_for_bert\"\n",
    "\n",
    "# Метки\n",
    "id2label = {0: \"CONTINUE\", 1: \"BEGIN\"}\n",
    "label2id = {\"CONTINUE\": 0, \"BEGIN\": 1}\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Приводит текст к нижнему регистру, убирает все, кроме кириллицы,\n",
    "    латиницы, цифр и пробелов, и нормализует пробелы.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Заменяет множественные пробелы на один\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# def clean_text(text: str) -> str:\n",
    "#     # Эта функция должна быть такой же, как при подготовке данных\n",
    "#     if not isinstance(text, str): return \"\"\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^а-яёa-z0-9\\s]', '', text)\n",
    "#     return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Эта функция должна быть такой же, как при подготовке данных\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^а-яёa-z0-9\\s]', '', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "print(\"Конфигурация и функция очистки готовы.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825134f",
   "metadata": {},
   "source": [
    "### Шаг 3: Функция для токенизации и выравнивания меток\n",
    "\n",
    "Эта функция подготавливает данные для модели токен-классификации (например, для задачи сегментации слов). Её главная задача — корректно сопоставить метки (в данном случае, \"начало слова\") с токенами, которые генерирует токенизатор, особенно когда одно слово разбивается на несколько саб-токенов.\n",
    "\n",
    "**Алгоритм работы:**\n",
    "\n",
    "1.  **Очистка и токенизация:**\n",
    "    *   Текст очищается от лишних символов, и из него **удаляются все пробелы**.\n",
    "    *   Полученный \"слитный\" текст токенизируется с запросом `offset_mapping` — карты, связывающей каждый токен с его позицией (индексами символов) в строке без пробелов.\n",
    "\n",
    "2.  **Создание карты меток на уровне символов:**\n",
    "    *   Для каждого примера создаётся \"идеальная\" разметка на уровне символов. В строке без пробелов `1` помечается только тот символ, с которого начинается новое слово, остальные помечаются `0`.\n",
    "\n",
    "3.  **Перенос меток на токены:**\n",
    "    *   С помощью `offset_mapping` функция определяет метку для каждого токена.\n",
    "    *   **Ключевая логика:** метка токена определяется меткой его **первого** символа. Если первый символ токена — это начало слова (метка `1`), то и весь токен получает метку `1`. В противном случае — `0`.\n",
    "    *   Специальные токены (типа `[CLS]`, `[SEP]`) получают метку `-100`, чтобы они игнорировались при обучении модели.\n",
    "\n",
    "В итоге функция возвращает словарь `tokenized_inputs`, дополненный ключом `labels` с выровненными метками.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e00c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Финальная, 100% корректная функция обработки готова.\n"
     ]
    }
   ],
   "source": [
    "# Шаг 3: Функция для токенизации и выравнивания меток (ФИНАЛЬНАЯ, ВЕРНАЯ ВЕРСИЯ)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Очищаем тексты и работаем только с ними\n",
    "    cleaned_texts_with_spaces = [clean_text(q) for q in examples[\"query\"]]\n",
    "    spaceless_texts = [q.replace(\" \", \"\") for q in cleaned_texts_with_spaces]\n",
    "\n",
    "    # Токенизируем, ЗАПРАШИВАЯ СМЕЩЕНИЯ (offsets)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        spaceless_texts,\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        is_split_into_words=False\n",
    "    )\n",
    "    \n",
    "    all_labels = []\n",
    "    \n",
    "    for i in range(len(spaceless_texts)):\n",
    "        # --- ШАГ 1: Создание идеальной карты меток на уровне символов ---\n",
    "        text_with_spaces = cleaned_texts_with_spaces[i]\n",
    "        text_spaceless = spaceless_texts[i]\n",
    "        char_labels = [0] * len(text_spaceless)\n",
    "        ptr = 0\n",
    "        for word in text_with_spaces.split():\n",
    "            # Находим, где начинается каждое слово в слитной строке\n",
    "            if text_spaceless[ptr:].startswith(word):\n",
    "                char_labels[ptr] = 1 # Помечаем эту позицию как \"начало\"\n",
    "                ptr += len(word)\n",
    "        \n",
    "        # --- ШАГ 2: Перенос меток с символов на токены с помощью offset_mapping ---\n",
    "        labels = []\n",
    "        offsets = tokenized_inputs.offset_mapping[i]\n",
    "\n",
    "        for start, end in offsets:\n",
    "            # Если это спец. токен ([CLS], [SEP]), его offset будет (0, 0)\n",
    "            if start == end:\n",
    "                labels.append(-100)\n",
    "                continue\n",
    "\n",
    "            # --- ГЛАВНАЯ ЛОГИКА ---\n",
    "            # Метка токена определяется меткой его ПЕРВОГО символа.\n",
    "            token_label = char_labels[start]\n",
    "            \n",
    "            if token_label == 0:\n",
    "                labels.append(0)\n",
    "            else: # token_label == 1\n",
    "                labels.append(1)\n",
    "        \n",
    "        all_labels.append(labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "print(\"Финальная, 100% корректная функция обработки готова.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d1cf21",
   "metadata": {},
   "source": [
    "### Шаг 4: Запуск процесса подготовки данных\n",
    "\n",
    "Этот блок кода выполняет полный цикл подготовки данных, превращая исходный сырой датасет в готовый для обучения модели.\n",
    "\n",
    "**Ключевые этапы:**\n",
    "\n",
    "1.  **Загрузка:** Загружается обучающая часть (`train`) исходного датасета по имени, указанному в `SOURCE_DATASET_NAME`.\n",
    "2.  **Фильтрация:** Из датасета удаляются все примеры, которые состоят из одного слова или менее. Это необходимо, так как задача сегментации для них нерелевантна.\n",
    "3.  **Обработка:** К отфильтрованным данным применяется основная функция `tokenize_and_align_labels` (из Шага 3) в пакетном режиме (`batched=True`) для эффективности. На этом шаге происходит токенизация и выравнивание меток.\n",
    "4.  **Сохранение:** Готовый, обработанный датасет сохраняется на диск в директорию `OUTPUT_DATASET_DIR`. Это позволяет не повторять трудоемкий процесс подготовки при последующих запусках.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd07f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94766ff4b1f84fa38a2441bbaaac2789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/32198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Шаг 4: Запуск процесса подготовки данных\n",
    "raw_dataset = load_dataset(SOURCE_DATASET_NAME, split='train')\n",
    "raw_dataset = raw_dataset.filter(lambda x: len(x['query'].split()) > 1)\n",
    "processed_dataset = raw_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "# Сохраняем на диск для дальнейшего использования\n",
    "processed_dataset.save_to_disk(OUTPUT_DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aace50",
   "metadata": {},
   "source": [
    "### Шаг 5: Проверка и верификация результата\n",
    "\n",
    "Этот блок кода предназначен для ручной проверки и визуализации результата обработки данных, выполненной на предыдущем шаге. Он позволяет убедиться, что сложная логика токенизации и выравнивания меток отработала корректно на конкретном примере.\n",
    "\n",
    "**Процесс проверки:**\n",
    "\n",
    "1.  **Загрузка:** Обработанный и сохраненный на диске датасет (`check_dataset`) загружается в память.\n",
    "2.  **Выбор примера:** Из датасета выбирается один конкретный пример для детального анализа.\n",
    "3.  **Визуализация:** Для выбранного примера выполняется \"обратное преобразование\":\n",
    "    *   `input_ids` декодируются обратно в читаемые токены.\n",
    "    *   Создается наглядная таблица, где каждому токену сопоставляется его числовая метка (`label`) и ее расшифровка (например, `1` — начало слова, `0` — продолжение, `-100` — игнорируемый спец. токен).\n",
    "4.  **Реконструкция:** На основе меток (`label == 1`) из последовательности токенов восстанавливается исходная строка с пробелами. Это служит финальным доказательством того, что разметка была присвоена правильно.\n",
    "*   `PROCESSED_DATASET_DIR`: Путь к предобработанному датасету.\n",
    "\n",
    "Цель ячейки — наглядно продемонстрировать, что данные подготовлены верно и готовы к подаче в модель для обучения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8c775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем сохраненный датасет для проверки...\n",
      "\n",
      "--- Пример обработанных данных ---\n",
      "Исходный запрос: 'контроллер полива'\n",
      "Токен                | ID метки | Расшифровка\n",
      "--------------------------------------------------\n",
      "[CLS]                | -100       | IGNORE\n",
      "мебель               | 1          | BEGIN\n",
      "##д                  | 1          | BEGIN\n",
      "##ля                 | 0          | CONTINUE\n",
      "##дет                | 1          | BEGIN\n",
      "##ской               | 0          | CONTINUE\n",
      "##комнат             | 1          | BEGIN\n",
      "##ын                 | 0          | CONTINUE\n",
      "##ед                 | 0          | CONTINUE\n",
      "##оро                | 0          | CONTINUE\n",
      "##го                 | 0          | CONTINUE\n",
      "##дос                | 1          | BEGIN\n",
      "##тав                | 0          | CONTINUE\n",
      "##ка                 | 0          | CONTINUE\n",
      "##уф                 | 1          | BEGIN\n",
      "##а                  | 0          | CONTINUE\n",
      "[SEP]                | -100       | IGNORE\n",
      "--------------------------------------------------\n",
      "Восстановленная строка (для проверки):\n",
      "'мебель для детской комнатынедорого доставка уфа'\n",
      "\n",
      "✅ Вывод выглядит корректно. Данные готовы для обучения.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ШАГ 5: ВЕРИФИКАЦИЯ ОБРАБОТАННЫХ ДАННЫХ\n",
    "# ==============================================================================\n",
    "PROCESSED_DATASET_DIR = \"./processed_dataset_for_bert\"\n",
    "\n",
    "print(\"Загружаем сохраненный датасет для проверки...\")\n",
    "check_dataset = Dataset.load_from_disk(PROCESSED_DATASET_DIR)\n",
    "\n",
    "# Возьмем любой пример для проверки\n",
    "sample_idx = 42 \n",
    "sample = check_dataset[sample_idx]\n",
    "original_query = load_dataset(SOURCE_DATASET_NAME, split='train')[sample_idx]['query']\n",
    "\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(sample[\"input_ids\"])\n",
    "labels = sample[\"labels\"]\n",
    "\n",
    "print(\"\\n--- Пример обработанных данных ---\")\n",
    "print(f\"Исходный запрос: '{original_query}'\")\n",
    "print(f\"{'Токен':<20} | {'ID метки'} | {'Расшифровка'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "reconstructed_words = []\n",
    "current_word = \"\"\n",
    "\n",
    "for token, label in zip(tokens, labels):\n",
    "    if label == -100:\n",
    "        label_name = \"IGNORE\"\n",
    "    else:\n",
    "        label_name = id2label[label]\n",
    "        # Собираем слова для наглядности\n",
    "        if label == 1 and current_word: # Если началось новое слово, сохраняем старое\n",
    "            reconstructed_words.append(current_word)\n",
    "            current_word = \"\"\n",
    "        # Удаляем префикс '##' у subword-токенов для чистоты\n",
    "        current_word += token.replace(\"##\", \"\")\n",
    "\n",
    "    print(f\"{token:<20} | {str(label):<10} | {label_name}\")\n",
    "\n",
    "# Добавляем последнее слово\n",
    "if current_word:\n",
    "    reconstructed_words.append(current_word)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Восстановленная строка (для проверки):\")\n",
    "# Убираем спец. токены из восстановленной строки\n",
    "reconstructed_query = \" \".join(reconstructed_words).replace('[CLS]', '').replace('[SEP]', '').strip()\n",
    "print(f\"'{reconstructed_query}'\")\n",
    "\n",
    "print(\"\\n✅ Вывод выглядит корректно. Данные готовы для обучения.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8da1c",
   "metadata": {},
   "source": [
    "### Шаг 2: Настройка конфигурации и гиперпараметров\n",
    "\n",
    "Этот блок кода выполняет всю необходимую настройку перед запуском обучения модели. Здесь определяются ключевые константы, пути к данным и модели, а также параметры самого процесса обучения.\n",
    "\n",
    "**Основные настройки:**\n",
    "\n",
    "1.  **Пути и модели:**\n",
    "    *   `MODEL_CHECKPOINT`: Имя базовой модели из Hugging Face Hub, которая будет дообучаться (`cointegrated/rubert-tiny2`).\n",
    "    *   `OUTPUT_MODEL_DIR`: Директория для сохранения обученной модели.\n",
    "\n",
    "2.  **Гиперпараметры обучения:**\n",
    "    *   `EPOCHS`: Количество эпох обучения.\n",
    "    *   `LEARNING_RATE`: Скорость обучения (learning rate) для оптимизатора.\n",
    "    *   `BATCH_SIZE`: Размер пакета данных, обрабатываемого за одну итерацию.\n",
    "\n",
    "3.  **Планировщик (Scheduler):**\n",
    "    *   Импортируется `get_linear_schedule_with_warmup`.\n",
    "    *   `WARMUP_RATIO`: Устанавливается доля \"прогревочных\" шагов, в течение которых скорость обучения плавно увеличивается. Это помогает стабилизировать обучение на начальном этапе.\n",
    "\n",
    "4.  **Выбор устройства:**\n",
    "    *   Код автоматически определяет наиболее производительное доступное устройство (`NVIDIA GPU`, `Apple Silicon GPU` или `CPU`) и выводит сообщение о своем выборе. Это делает скрипт переносимым и эффективным на разном оборудовании.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0bf7001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется Apple Silicon GPU (mps)\n",
      "✅ Конфигурация настроена.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Шаг 2: Настройка конфигурации и гиперпараметров (с scheduler)\n",
    "# ==============================================================================\n",
    "\n",
    "# Добавляем импорт scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Пути и имена\n",
    "MODEL_CHECKPOINT = \"cointegrated/rubert-tiny2\"\n",
    "OUTPUT_MODEL_DIR = \"./rubert_tiny_spaced_restoration\"\n",
    "\n",
    "# Гиперпараметры обучения\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Параметры для scheduler\n",
    "WARMUP_RATIO = 0.1  # Доля шагов для warm-up (обычно 10%)\n",
    "\n",
    "# Определяем устройство (GPU, если доступно, иначе CPU)\n",
    "\n",
    "# ==============================================================================\n",
    "# Определяем устройство (GPU, если доступно, иначе CPU)\n",
    "# Это правильный способ для поддержки и NVIDIA (cuda), и Apple Silicon (mps)\n",
    "# ==============================================================================\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Используется NVIDIA GPU (cuda)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Используется Apple Silicon GPU (mps)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Используется CPU\")\n",
    "print(\"✅ Конфигурация настроена.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca88bee",
   "metadata": {},
   "source": [
    "### Шаг 3: Загрузка данных и модели\n",
    "\n",
    "Этот блок кода подготавливает все необходимые компоненты для начала процесса обучения.\n",
    "\n",
    "**Ключевые действия:**\n",
    "\n",
    "1.  **Загрузка и разделение датасета:**\n",
    "    *   Загружается предобработанный на предыдущих этапах датасет с диска.\n",
    "    *   Он разделяется на обучающую (`train`) и валидационную (`test`) выборки в соотношении 95/5. Использование `seed=42` гарантирует воспроизводимость этого разделения.\n",
    "\n",
    "2.  **Загрузка токенизатора и модели:**\n",
    "    *   Загружается токенизатор, соответствующий выбранной архитектуре модели (`MODEL_CHECKPOINT`).\n",
    "    *   Загружается сама модель (`AutoModelForTokenClassification`), специально сконфигурированная для задачи классификации токенов. Ей передаются словари `id2label` и `label2id`, чтобы она корректно интерпретировала метки классов.\n",
    "\n",
    "3.  **Перемещение на устройство:**\n",
    "    *   Загруженная модель перемещается на ранее определенное устройство (`cuda`, `mps` или `cpu`) для выполнения вычислений. Это критически важно для ускорения обучения на GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccbe6625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасеты загружены. Обучение: 30588, Валидация: 1610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Токенизатор и модель загружены и перемещены на устройство.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Шаг 3: Загрузка данных и модели\n",
    "# ==============================================================================\n",
    "\n",
    "# Загружаем датасет\n",
    "full_dataset = load_from_disk(PROCESSED_DATASET_DIR)\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "print(f\"Датасеты загружены. Обучение: {len(train_dataset)}, Валидация: {len(eval_dataset)}\")\n",
    "\n",
    "# Загружаем токенизатор и модель\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Перемещаем модель на нужное устройство\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "print(\"✅ Токенизатор и модель загружены и перемещены на устройство.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bbb6ac",
   "metadata": {},
   "source": [
    "### Шаг 4: Подготовка DataLoaders, оптимизатора и scheduler\n",
    "\n",
    "Этот блок кода выполняет финальную подготовку компонентов, необходимых для организации цикла обучения модели.\n",
    "\n",
    "**Ключевые этапы:**\n",
    "\n",
    "1.  **Очистка датасетов:**\n",
    "    *   Из обучающего и валидационного датасетов удаляются все колонки, которые не требуются модели напрямую во время обучения (`input_ids`, `attention_mask`, `labels`).\n",
    "    *   Важная информация для этапа оценки (`offset_mapping` и `query`) предварительно сохраняется в отдельные списки, так как она понадобится для анализа результатов, но мешает автоматическому созданию батчей.\n",
    "\n",
    "2.  **`Data Collator`:**\n",
    "    *   Инициализируется `DataCollatorForTokenClassification`. Его задача — принимать список примеров из датасета и грамотно объединять их в один пакет (батч), добавляя паддинг до максимальной длины в этом батче и преобразуя все в тензоры PyTorch (`pt`).\n",
    "\n",
    "3.  **`DataLoaders`:**\n",
    "    *   Создаются итераторы `DataLoader` для обучающей и валидационной выборок. Они будут подавать данные в модель батчами. Для обучающего датасета включено перемешивание (`shuffle=True`) для лучшего обучения.\n",
    "\n",
    "4.  **Оптимизатор и Планировщик (`Scheduler`):**\n",
    "    *   В качестве оптимизатора выбран `AdamW`, стандартный для современных трансформеров.\n",
    "    *   Настраивается планировщик `get_cosine_schedule_with_warmup`. Он будет динамически изменять скорость обучения (`learning rate`): сначала плавно повышать её на протяжении `warmup_steps`, а затем медленно снижать по косинусоиде. Это помогает стабилизировать обучение и улучшить итоговое качество модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33eb2c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Колонки в обработанном датасете: ['input_ids', 'attention_mask', 'labels']\n",
      "✅ DataLoaders и оптимизатор готовы. Общее число шагов: 19120, из них warmup: 1912\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Шаг 4 (исправленный): Подготовка DataLoaders, оптимизатора и scheduler\n",
    "# ==============================================================================\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "# Удаляем ненужные для модели поля из датасета\n",
    "columns_to_keep = ['input_ids', 'attention_mask', 'labels']\n",
    "columns_to_remove = [col for col in train_dataset.column_names if col not in columns_to_keep]\n",
    "\n",
    "# Для оценки нам понадобятся offset_mapping и query, сохраняем их отдельно\n",
    "eval_offsets = [example['offset_mapping'] for example in eval_dataset]\n",
    "eval_queries = [example['query'] for example in eval_dataset]\n",
    "\n",
    "# Удаляем лишние колонки из обоих датасетов\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "eval_dataset = eval_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "print(f\"Колонки в обработанном датасете: {train_dataset.column_names}\")\n",
    "\n",
    "# Data Collator с указанием правильных параметров\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, \n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Создаем DataLoader-ы\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Оптимизатор AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Рассчитываем общее количество шагов обучения и шагов warm-up\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "print(f\"✅ DataLoaders и оптимизатор готовы. Общее число шагов: {total_steps}, из них warmup: {warmup_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d8de6",
   "metadata": {},
   "source": [
    "### Шаг 5: Функции для расчета метрик\n",
    "\n",
    "Этот блок содержит все необходимые функции для комплексной оценки производительности модели по расстановке пробелов. Его ключевая особенность — расчет двух разных типов метрик: стандартной для задач токен-классификации и кастомной, которая точно соответствует бизнес-задаче (правильность расстановки пробелов).\n",
    "\n",
    "**Составные части:**\n",
    "\n",
    "1.  **Вспомогательные функции:**\n",
    "    *   `get_true_positions`: Определяет \"идеальные\" позиции пробелов. Функция принимает исходный текст, удаляет из него пробелы и возвращает множество индексов, где в слитной строке должен стоять пробел.\n",
    "    *   `get_predicted_positions`: Определяет предсказанные моделью позиции пробелов. Она анализирует выходы модели, и для каждого токена с меткой `1` (начало слова) извлекает его начальную позицию в слитной строке с помощью `offset_mapping`.\n",
    "\n",
    "2.  **Главная функция оценки (`run_evaluation_and_print_metrics`):**\n",
    "    Эта функция запускает и координирует весь процесс оценки. Она вычисляет и выводит два типа метрик:\n",
    "    *   **Метрика на уровне токенов (`seqeval`):** Стандартная метрика для задач токен-классификации. Она сравнивает предсказанные и истинные метки токенов (`BEGIN`, `CONTINUE`) и полезна для общей оценки качества классификации на низком уровне.\n",
    "    *   **Метрика по позициям пробелов:** Ключевая, более практичная метрика. Она напрямую оценивает, насколько правильно модель расставила пробелы, сравнивая множества \"истинных\" и \"предсказанных\" позиций. Метрики (Precision, Recall, F1) вычисляются для каждого примера отдельно, а затем усредняются по всему датасету.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83410544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Функции для расчета метрик готовы.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Шаг 5: Функции для расчета метрик (ФИНАЛЬНАЯ, УЛУЧШЕННАЯ ВЕРСИЯ)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ ---\n",
    "\n",
    "\n",
    "def get_true_positions(text_with_spaces: str) -> set:\n",
    "    \"\"\"Извлекает множество истинных позиций пробелов.\"\"\"\n",
    "    clean = clean_text(text_with_spaces)\n",
    "    spaceless_text = clean.replace(\" \", \"\")\n",
    "    positions = set()\n",
    "    current_pos = 0\n",
    "    for word in clean.split():\n",
    "        current_pos += len(word)\n",
    "        if current_pos < len(spaceless_text):\n",
    "            positions.add(current_pos)\n",
    "    return positions\n",
    "\n",
    "def get_predicted_positions(predictions, true_labels, offsets) -> set:\n",
    "    \"\"\"Извлекает множество предсказанных позиций пробелов.\"\"\"\n",
    "    positions = set()\n",
    "    for i in range(len(true_labels)):\n",
    "        if true_labels[i] != -100 and predictions[i] == 1 and offsets[i][0] > 0:\n",
    "            positions.add(offsets[i][0])\n",
    "    return positions\n",
    "\n",
    "\n",
    "# --- ГЛАВНАЯ ФУНКЦИЯ ОЦЕНКИ ---\n",
    "\n",
    "def run_evaluation_and_print_metrics(all_predictions, all_true_labels, eval_dataset_full):\n",
    "    \"\"\"\n",
    "    Рассчитывает и выводит обе метрики: seqeval и F1 по позициям пробелов.\n",
    "    \"\"\"\n",
    "    print(\"\\n  ---- Результаты оценки ----\")\n",
    "    \n",
    "    # 1. Метрика по токенам (seqeval) для отладки\n",
    "    print(\"  Метрики по токенам (seqeval):\")\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    id2label = {0: \"CONTINUE\", 1: \"BEGIN\"}\n",
    "    true_preds_str = [[id2label[p] for p, l in zip(pred, lbl) if l != -100] for pred, lbl in zip(all_predictions, all_true_labels)]\n",
    "    true_lbls_str = [[id2label[l] for _, l in zip(pred, lbl) if l != -100] for pred, lbl in zip(all_predictions, all_true_labels)]\n",
    "    seqeval_results = metric.compute(predictions=true_preds_str, references=true_lbls_str, zero_division=0)\n",
    "    print(f\"    - Precision: {seqeval_results['overall_precision']:.4f}, Recall: {seqeval_results['overall_recall']:.4f}, F1: {seqeval_results['overall_f1']:.4f}\")\n",
    "    \n",
    "    # 2. Метрика по позициям пробелов (как на Stepik)\n",
    "    print(\"\\n  Метрика по позициям пробелов (как на платформе):\")\n",
    "    all_f1, all_precision, all_recall = [], [], []\n",
    "\n",
    "    for i in range(len(all_predictions)):\n",
    "        # Получаем все данные для i-го примера\n",
    "        prediction = all_predictions[i]\n",
    "        true_label = all_true_labels[i]\n",
    "        # `query` и `offset_mapping` берем из полного eval_dataset\n",
    "        original_sample = eval_dataset_full[i]\n",
    "        \n",
    "        true_pos = get_true_positions(original_sample['query'])\n",
    "        pred_pos = get_predicted_positions(prediction, true_label, original_sample['offset_mapping'])\n",
    "        \n",
    "        # Считаем Precision, Recall, F1 для одного примера\n",
    "        tp = len(true_pos.intersection(pred_pos))\n",
    "        precision = tp / len(pred_pos) if len(pred_pos) > 0 else 0.0\n",
    "        recall = tp / len(true_pos) if len(true_pos) > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        all_f1.append(f1)\n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "        \n",
    "    # Усредняем метрики по всему датасету\n",
    "    mean_precision = np.mean(all_precision)\n",
    "    mean_recall = np.mean(all_recall)\n",
    "    mean_f1 = np.mean(all_f1)\n",
    "    \n",
    "    print(f\"    - Precision: {mean_precision:.4f}\")\n",
    "    print(f\"    - Recall:    {mean_recall:.4f}\")\n",
    "    print(f\"    - F1-score:  {mean_f1:.4f} (или {mean_f1 * 100:.2f}%)\")\n",
    "    print(\"  -------------------------\")\n",
    "\n",
    "print(\"✅ Функции для расчета метрик готовы.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab820fb9",
   "metadata": {},
   "source": [
    "### Шаг 4.5: Расчет весов классов для борьбы с дисбалансом\n",
    "\n",
    "Этот блок кода решает распространенную проблему дисбаланса классов в задачах токен-классификации, где одни метки (например, \"продолжение слова\", `0`) встречаются гораздо чаще других (\"начало слова\", `1`). Без корректировки модель может научиться в основном предсказывать частый класс, игнорируя редкий.\n",
    "\n",
    "**Алгоритм действий:**\n",
    "\n",
    "1.  **Подсчет классов:** Код итерирует по всему обучающему датасету и точно подсчитывает общее количество меток `0` и `1`.\n",
    "\n",
    "2.  **Расчет весов:** Веса для каждого класса вычисляются по принципу **обратной пропорциональности**:\n",
    "    *   Класс, который встречается реже (метка `1`), получает больший вес.\n",
    "    *   Класс, который встречается чаще (метка `0`), получает меньший вес.\n",
    "\n",
    "3.  **Нормализация:** Рассчитанные веса дополнительно нормализуются. Это помогает сохранить стабильность процесса обучения, не давая значениям потерь становиться слишком большими.\n",
    "\n",
    "4.  **Создание функции потерь:**\n",
    "    *   Вычисленные веса преобразуются в тензор `torch`.\n",
    "    *   Создается кастомная функция потерь `CrossEntropyLoss`, которой передается этот тензор весов. Теперь, при расчете ошибки, неправильное предсказание редкого (но важного) класса `1` будет \"штрафоваться\" сильнее, заставляя модель уделять ему больше внимания.\n",
    "    *   Параметр `ignore_index=-100` гарантирует, что специальные токены не будут учитываться при расчете потерь.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebc5ebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚖️ Рассчитываем веса для классов 0 и 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1c5d89e8d64c8ba33bcf61fbc433aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Подсчет меток:   0%|          | 0/30588 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Найдено меток класса 0 (CONTINUE): 224089\n",
      "  Найдено меток класса 1 (BEGIN):    157727\n",
      "\n",
      "Рассчитанные веса: [вес_для_0, вес_для_1] = [0.82619375 1.1738062 ]\n",
      "\n",
      "✅ Функция потерь (Loss) с весами классов готова.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ШАГ 4.5: Расчет весов классов для борьбы с дисбалансом\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"⚖️ Рассчитываем веса для классов 0 и 1...\")\n",
    "\n",
    "# Пройдемся по всему обучающему датасету и посчитаем количество меток\n",
    "num_labels_0 = 0\n",
    "num_labels_1 = 0\n",
    "\n",
    "for example in tqdm(train_dataset, desc=\"Подсчет меток\"):\n",
    "    for label in example['labels']:\n",
    "        if label == 0:\n",
    "            num_labels_0 += 1\n",
    "        elif label == 1:\n",
    "            num_labels_1 += 1\n",
    "\n",
    "print(f\"  Найдено меток класса 0 (CONTINUE): {num_labels_0}\")\n",
    "print(f\"  Найдено меток класса 1 (BEGIN):    {num_labels_1}\")\n",
    "num_labels_0 *= 1\n",
    "num_labels_1 *= 1\n",
    "# Расчет весов (обратно пропорционально частоте)\n",
    "total_labels = num_labels_0 + num_labels_1 \n",
    "weight_0 = total_labels / num_labels_0 \n",
    "weight_1 = total_labels / num_labels_1\n",
    "\n",
    "# Нормализуем веса, чтобы они не были слишком большими\n",
    "# (например, чтобы их сумма была равна количеству классов)\n",
    "norm_factor = 2 / (weight_0 + weight_1)\n",
    "weight_0 *= norm_factor\n",
    "weight_1 *= norm_factor\n",
    "\n",
    "# Создаем тензор весов, который передадим в функцию потерь\n",
    "class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float).to(device)\n",
    "\n",
    "print(f\"\\nРассчитанные веса: [вес_для_0, вес_для_1] = {class_weights.cpu().numpy()}\")\n",
    "\n",
    "# Создаем кастомную функцию потерь с нашими весами\n",
    "# `ignore_index=-100` автоматически игнорирует все метки -100\n",
    "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "\n",
    "print(\"\\n✅ Функция потерь (Loss) с весами классов готова.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44098377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Шаг 6: Ручной цикл обучения и оценки (с scheduler и validation loss)\n",
    "# ==============================================================================\n",
    "print(\"🚀 Начинаем обучение...\")\n",
    "\n",
    "# Отслеживаем лучшие метрики для сохранения лучшей модели\n",
    "best_f1 = 0.0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Для отслеживания динамики обучения\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "f1_scores = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n===== Эпоха {epoch + 1}/{EPOCHS} =====\")\n",
    "    \n",
    "    # Выводим текущий learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"  📊 Текущий Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # --- Тренировка ---\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_dataloader, desc=\"Тренировка\"):\n",
    "        # Очищаем градиенты\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Перемещаем все входные данные на GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Получаем метки из батча\n",
    "        labels = batch.pop(\"labels\")\n",
    "        \n",
    "        # Прямой проход\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Вычисление функции потерь\n",
    "        loss = loss_function(logits.view(-1, model.num_labels), labels.view(-1))\n",
    "        \n",
    "        # Сохраняем значение потерь\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Обратное распространение и оптимизация\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Обновляем learning rate\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Средняя ошибка на тренировке\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"  Средняя ошибка на тренировке (Train Loss): {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # --- Оценка ---\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    total_val_loss = 0  # Добавляем счетчик для validation loss\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Оценка\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop(\"labels\")\n",
    "            \n",
    "            # Получаем предсказания и loss\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Считаем validation loss\n",
    "            val_loss = loss_function(logits.view(-1, model.num_labels), labels.view(-1))\n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "            # Получаем предсказанные классы\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Переносим данные на CPU для дальнейшей обработки\n",
    "            predictions = predictions.detach().cpu().numpy()\n",
    "            true_labels = labels.detach().cpu().numpy()\n",
    "            \n",
    "            # Сохраняем предсказания и метки\n",
    "            for i in range(len(predictions)):\n",
    "                all_predictions.append(predictions[i])\n",
    "                all_true_labels.append(true_labels[i])\n",
    "    \n",
    "    # Средняя ошибка на валидации\n",
    "    avg_val_loss = total_val_loss / len(eval_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"  Средняя ошибка на валидации (Val Loss): {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # --- Расчет метрик ---\n",
    "    print(\"\\n  ---- Результаты оценки ----\")\n",
    "    \n",
    "    # 1. Метрика seqeval\n",
    "    print(\"  Метрики по токенам (seqeval):\")\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    \n",
    "    true_preds_str = []\n",
    "    true_lbls_str = []\n",
    "    \n",
    "    for pred, lbl in zip(all_predictions, all_true_labels):\n",
    "        pred_str = [id2label[p] for p, l in zip(pred, lbl) if l != -100]\n",
    "        lbl_str = [id2label[l] for l in lbl if l != -100]\n",
    "        true_preds_str.append(pred_str)\n",
    "        true_lbls_str.append(lbl_str)\n",
    "    \n",
    "    seqeval_results = metric.compute(predictions=true_preds_str, references=true_lbls_str, zero_division=0)\n",
    "    print(f\"    - Precision: {seqeval_results['overall_precision']:.4f}\")\n",
    "    print(f\"    - Recall: {seqeval_results['overall_recall']:.4f}\")\n",
    "    print(f\"    - F1: {seqeval_results['overall_f1']:.4f}\")\n",
    "    \n",
    "    # 2. Метрика по позициям пробелов\n",
    "    print(\"\\n  Метрика по позициям пробелов:\")\n",
    "    all_f1, all_precision, all_recall = [], [], []\n",
    "    \n",
    "    for i in range(len(all_predictions)):\n",
    "        # Используем сохраненные ранее offsets и queries\n",
    "        true_pos = get_true_positions(eval_queries[i])\n",
    "        pred_pos = get_predicted_positions(all_predictions[i], all_true_labels[i], eval_offsets[i])\n",
    "        \n",
    "        # Рассчет метрик для примера\n",
    "        tp = len(true_pos.intersection(pred_pos))\n",
    "        precision = tp / len(pred_pos) if len(pred_pos) > 0 else 0.0\n",
    "        recall = tp / len(true_pos) if len(true_pos) > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "        all_f1.append(f1)\n",
    "    \n",
    "    # Средние метрики\n",
    "    mean_precision = np.mean(all_precision)\n",
    "    mean_recall = np.mean(all_recall)\n",
    "    mean_f1 = np.mean(all_f1)\n",
    "    f1_scores.append(mean_f1)\n",
    "    \n",
    "    print(f\"    - Precision: {mean_precision:.4f}\")\n",
    "    print(f\"    - Recall:    {mean_recall:.4f}\")\n",
    "    print(f\"    - F1-score:  {mean_f1:.4f} (или {mean_f1 * 100:.2f}%)\")\n",
    "    print(\"  -------------------------\")\n",
    "\n",
    "    # Сохраняем модель по лучшему F1-score\n",
    "    if mean_f1 > best_f1:\n",
    "        best_f1 = mean_f1\n",
    "        save_path = f\"{OUTPUT_MODEL_DIR}_best_f1\"\n",
    "        model.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        print(f\"\\n🏆 Новый лучший F1-score: {best_f1:.4f}! Модель сохранена в: '{save_path}'\")\n",
    "\n",
    "    # Сохраняем модель по лучшему validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        save_path = f\"{OUTPUT_MODEL_DIR}_best_loss\"\n",
    "        model.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        print(f\"\\n📉 Новая лучшая ошибка (Val Loss): {best_val_loss:.4f}! Модель сохранена в: '{save_path}'\")\n",
    "\n",
    "    # Сохраняем модель если это последняя эпоха\n",
    "    if epoch + 1 == EPOCHS:\n",
    "        model.save_pretrained(OUTPUT_MODEL_DIR)\n",
    "        tokenizer.save_pretrained(OUTPUT_MODEL_DIR)\n",
    "        print(f\"\\n✅ Финальная модель сохранена в директории: '{OUTPUT_MODEL_DIR}'\")\n",
    "        \n",
    "    # Оцениваем признаки переобучения\n",
    "    if epoch > 3 and avg_train_loss < avg_val_loss * 0.7:\n",
    "        print(\"\\n⚠️ Возможные признаки переобучения: Train Loss намного ниже, чем Val Loss\")\n",
    "    \n",
    "    # Досрочное завершение при очень высоком F1-score\n",
    "    if mean_f1 > 0.98:\n",
    "        print(\"\\n🎯 Достигнут отличный результат! Досрочно завершаем обучение.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n🎉 Обучение завершено!\")\n",
    "print(f\"   - Лучший F1-score: {best_f1:.4f}\")\n",
    "print(f\"   - Лучший Val Loss: {best_val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
